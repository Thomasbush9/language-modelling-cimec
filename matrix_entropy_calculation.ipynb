{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef4c7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_and_prepare_data(file_path, tokenizer):\n",
    "    df = pd.read_csv(file_path)\n",
    "    texts = df['input'].tolist()  # Replace 'input_column_name' with your actual text column name\n",
    "    # Tokenization to create a dictionary of tensors\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9a8d5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prepare_data(file_path, tokenizer, device, fraction=0.005):\n",
    "    df = pd.read_csv(file_path)\n",
    "    if not df['input'].dropna().shape[0] == df.shape[0]:\n",
    "        raise ValueError(\"Data contains NaN values.\")\n",
    "    \n",
    "    # Filter out empty strings or possibly problematic texts\n",
    "    df = df[df['input'].str.strip() != \"\"]\n",
    "    if df.empty:\n",
    "        raise ValueError(\"All data filtered out after removing empty inputs.\")\n",
    "\n",
    "    df_sampled = df.sample(frac=fraction, random_state=42)\n",
    "    texts = df_sampled['input'].tolist()\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Diagnostic to check tokenization results\n",
    "    if (inputs['input_ids'] == 0).all():\n",
    "        raise ValueError(\"Tokenization resulted in all zero input_ids, check tokenizer settings and inputs.\")\n",
    "    \n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0edc566",
   "metadata": {},
   "source": [
    "## Reviewed functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "80c4a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(R):\n",
    "    with torch.no_grad():\n",
    "        if R.dim() == 1:\n",
    "            R = R.unsqueeze(0)  # Add a batch dimension if R is 1-dimensional\n",
    "        mean = R.mean(dim=0)\n",
    "        R = R - mean\n",
    "        norms = torch.norm(R, p=2, dim=1, keepdim=True)\n",
    "        if any(norms == 0):\n",
    "            print(\"Zero norms detected, problematic tensor:\", R)\n",
    "            return None  # Return None if norms are zero (avoid division by zero)\n",
    "        R = R / norms\n",
    "        if not torch.isfinite(R).all():\n",
    "            print(\"Normalization resulted in non-finite values, input tensor:\", R)\n",
    "            return None\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3f1ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_integrity(inputs):\n",
    "    if not torch.isfinite(inputs['input_ids']).all():\n",
    "        print(\"Non-finite values found in input_ids:\", inputs['input_ids'])\n",
    "    if 'attention_mask' in inputs and not torch.isfinite(inputs['attention_mask']).all():\n",
    "        print(\"Non-finite values found in attention_mask:\", inputs['attention_mask'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a26f77d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_cov(R):\n",
    "    with torch.no_grad():\n",
    "        if R is None or R.nelement() == 0:\n",
    "            return None  # Return None if R is None or empty\n",
    "        Z = torch.nn.functional.normalize(R, dim=1)\n",
    "        if not torch.isfinite(Z).all():\n",
    "            return None  # Check if Z contains non-finite values\n",
    "        A = torch.matmul(Z.T, Z) / Z.shape[0]\n",
    "        if not torch.isfinite(A).all():\n",
    "            return None\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e27dd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_entropy(A):\n",
    "    with torch.no_grad():\n",
    "        if torch.trace(A) == 0:\n",
    "            raise ValueError(\"Trace of the matrix A is zero, which can cause division by zero in normalization.\")\n",
    "        normalized_A = A / torch.trace(A)\n",
    "        try:\n",
    "            eig_val = torch.svd(normalized_A).S\n",
    "        except RuntimeError as e:\n",
    "            print(\"SVD failed:\", e)\n",
    "            print(\"Matrix A:\", A)\n",
    "            raise\n",
    "\n",
    "        eig_val = eig_val[eig_val > 0]  # Remove zero or negative eigenvalues\n",
    "        entropy = - (eig_val * torch.log(eig_val)).sum().item()\n",
    "        normalized_entropy = entropy / math.log(A.shape[0])\n",
    "        normalized_entropy1 = math.exp(entropy) / A.shape[0]\n",
    "    return normalized_entropy, normalized_entropy1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "903179cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_matrix_entropy(inputs, model):\n",
    "    ls1, ls3 = [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        for hidden_state in last_hidden_states:\n",
    "            if torch.all(hidden_state == 0):\n",
    "                print(\"Skipping zero vector.\")\n",
    "                continue  # Skip this loop iteration if hidden_state is a zero vector\n",
    "            hidden_state = hidden_state.mean(dim=0, keepdim=True)\n",
    "            R = normalize(hidden_state)\n",
    "            if R is None:\n",
    "                continue  # Skip this batch if normalization failed\n",
    "            A = cal_cov(R)\n",
    "            if A is None:\n",
    "                continue  # Skip this batch if covariance calculation failed\n",
    "            entropy1, entropy3 = cal_entropy(A)\n",
    "            ls1.append(entropy1)\n",
    "            ls3.append(entropy3)\n",
    "\n",
    "    entropy_avg1 = sum(ls1) / len(ls1) if ls1 else float('nan')\n",
    "    entropy_avg3 = sum(ls3) / len(ls3) if ls3 else float('nan')\n",
    "    return entropy_avg1, entropy_avg3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8e932451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(data_loader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in data_loader:\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Ensure inputs are on the same device as model\n",
    "            output = model(**inputs)\n",
    "            # Extract embeddings from a specific layer, e.g., last hidden state or any intermediate layer\n",
    "            embeddings.append(output.last_hidden_state.mean(dim=1).detach().cpu().numpy())\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eaa3d867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_covariance_and_entropy(embeddings):\n",
    "    # Ensure embeddings are real numbers\n",
    "    embeddings = np.real(embeddings)\n",
    "\n",
    "    # Calculate the covariance matrix and ensure it's symmetric\n",
    "    covariance_matrix = np.cov(embeddings, rowvar=False)\n",
    "    covariance_matrix = (covariance_matrix + covariance_matrix.T) / 2\n",
    "\n",
    "    # Calculate eigenvalues\n",
    "    eigenvalues = np.linalg.eigvalsh(covariance_matrix)  # Use eigvalsh for Hermitian matrices\n",
    "    eigenvalues = eigenvalues[eigenvalues > 0]  # Filter out non-positive to avoid log of non-positive numbers\n",
    "\n",
    "    # Calculate entropy\n",
    "    entropy = -np.sum(eigenvalues * np.log(eigenvalues))\n",
    "    normalized_entropy = entropy / np.log(len(eigenvalues))\n",
    "\n",
    "    return covariance_matrix, entropy, normalized_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8fa65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def create_subset_loader(full_dataset, fraction=0.005):\n",
    "    # Determine the number of samples to include\n",
    "    subset_size = int(len(full_dataset) * fraction)\n",
    "    # Randomly sample indices for the subset\n",
    "    subset_indices = np.random.choice(len(full_dataset), subset_size, replace=False)\n",
    "    # Create a subset dataset\n",
    "    subset_dataset = Subset(full_dataset, subset_indices)\n",
    "    # Create a DataLoader for the subset dataset\n",
    "    subset_loader = DataLoader(subset_dataset, batch_size=32, shuffle=True)\n",
    "    return subset_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d847efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(model, data_loader):\n",
    "    embeddings = extract_embeddings(data_loader, model)\n",
    "    covariance_matrix, entropy, normalized_entropy = calculate_covariance_and_entropy(embeddings)\n",
    "    print(f\"Entropy: {entropy}, Normalized Entropy: {normalized_entropy}\")\n",
    "    return entropy, normalized_entropy\n",
    "\n",
    "# Example usage, assuming you have 'model' and 'data_loader' ready\n",
    "main(model, data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "511599a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "class DataFrameDataset(Dataset):\n",
    "    def __init__(self, dataframe, input_column, output_column=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (DataFrame): Source data\n",
    "            input_column (str): Column name for the model input data\n",
    "            output_column (str, optional): Column name for the model output/target data\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.input_column = input_column\n",
    "        self.output_column = output_column\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Fetch the input data using the specified column\n",
    "        input_data = self.dataframe.iloc[idx][self.input_column]\n",
    "        \n",
    "        # Process the input data through tokenizer or any preprocessing here if necessary\n",
    "        # Example: input_tensor = tokenizer.encode(input_data, ...)\n",
    "        # For simplicity, assume it's pre-processed or handled elsewhere\n",
    "\n",
    "        if self.output_column is not None:\n",
    "            # Fetch the output data if an output column is specified\n",
    "            output_data = self.dataframe.iloc[idx][self.output_column]\n",
    "            return input_data, output_data\n",
    "        \n",
    "        return input_data\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'df' is your pandas DataFrame with columns 'input' for inputs and 'output' for expected outputs\n",
    "df = pd.read_csv('/Users/thomasbush/langmodel/direct_data.csv')\n",
    "full_dataset = DataFrameDataset(df, 'input', 'output')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "83fa1097",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def extract_embeddings(data_loader, model):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            # Check if data is a tuple (input, output) and extract input part\n",
    "            if isinstance(data, tuple):\n",
    "                inputs = data[0]\n",
    "            else:\n",
    "                inputs = data\n",
    "\n",
    "            # Ensure inputs are tensors; convert if not\n",
    "            if not isinstance(inputs, torch.Tensor):\n",
    "                inputs = torch.tensor(inputs, dtype=torch.float32)  # You might need to adjust dtype based on your model's expected input\n",
    "                \n",
    "            # Move inputs to the correct device\n",
    "            inputs = inputs.to(model.device)\n",
    "            \n",
    "            # Forward pass to get outputs from the model\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Assuming you want to extract something specific from the outputs; adjust accordingly\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e445b6b1",
   "metadata": {},
   "source": [
    "### Logic Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3bfea8b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: -787.4395632753549, Normalized Entropy: -118.52264970798292\n"
     ]
    }
   ],
   "source": [
    "# Create a DataLoader for extracting embeddings (inputs only)\n",
    "subset_loader = create_subset_loader(full_dataset, fraction=1.0)\n",
    "\n",
    "def extract_embeddings(data_loader, model, tokenizer):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_texts, _ = data  # Unpack the tuple containing inputs and labels\n",
    "\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move tokenized inputs to the correct device\n",
    "\n",
    "            # Forward pass to get outputs from the model\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Extract embeddings; adjust according to what you actually need, here using the last hidden state\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "def main(model, data_loader, tokenizer):\n",
    "    embeddings = extract_embeddings(data_loader, model, tokenizer)\n",
    "    covariance_matrix, entropy, normalized_entropy = calculate_covariance_and_entropy(embeddings.detach().cpu().numpy())\n",
    "    print(f\"Entropy: {entropy}, Normalized Entropy: {normalized_entropy}\")\n",
    "\n",
    "# Assuming 'model' is already defined and properly configured\n",
    "model_path = \"/Users/thomasbush/langmodel/logic_model\"\n",
    "dataset_path = \"/Users/thomasbush/langmodel/direct_data.csv\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "main(model, subset_loader, tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ba81be",
   "metadata": {},
   "source": [
    "### Redundand Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "480d1a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy: 1.593753028287761, Normalized Entropy: 0.23988613311106072\n"
     ]
    }
   ],
   "source": [
    "subset_loader = create_subset_loader(full_dataset, fraction=1.0)\n",
    "\n",
    "def extract_embeddings(data_loader, model, tokenizer):\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            input_texts, _ = data  # Unpack the tuple containing inputs and labels\n",
    "\n",
    "            # Tokenize inputs\n",
    "            inputs = tokenizer(input_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move tokenized inputs to the correct device\n",
    "\n",
    "            # Forward pass to get outputs from the model\n",
    "            outputs = model(**inputs)\n",
    "            \n",
    "            # Extract embeddings; adjust according to what you actually need, here using the last hidden state\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "            embeddings.append(embedding)\n",
    "\n",
    "    # Concatenate all batch embeddings into a single tensor\n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    return embeddings\n",
    "\n",
    "# Example usage\n",
    "def main(model, data_loader, tokenizer):\n",
    "    embeddings = extract_embeddings(data_loader, model, tokenizer)\n",
    "    covariance_matrix, entropy, normalized_entropy = calculate_covariance_and_entropy(embeddings.detach().cpu().numpy())\n",
    "    print(f\"Entropy: {entropy}, Normalized Entropy: {normalized_entropy}\")\n",
    "\n",
    "# Assuming 'model' is already defined and properly configured\n",
    "model_path = \"/Users/thomasbush/langmodel/redu_model\"\n",
    "dataset_path = \"/Users/thomasbush/langmodel/redundant_data.csv\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "main(model, subset_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a321a9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (langmodel)",
   "language": "python",
   "name": "langmodel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "220px",
    "left": "1533px",
    "right": "20px",
    "top": "-11px",
    "width": "380px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
